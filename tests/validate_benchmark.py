"""
–í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ benchmark_3000.json
–ü—Ä–æ–≤–µ—Ä—è–µ—Ç: —Å—Ä–∞–±–æ—Ç–∞–ª–æ / –≤—ã–ø–æ–ª–Ω–∏–ª–æ—Å—å / –ø–æ—Ö–æ–∂–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
–ë–µ–∑ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
"""

import json
import logging
import time
import asyncio
from pathlib import Path
from typing import Dict, Any, List, Optional
import sys
from datetime import datetime
import pandas as pd
import numpy as np

# –î–æ–±–∞–≤–∏—Ç—å —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from agents.orchestrator import AgentOrchestrator
from core.database_adapter import create_database_adapter
from core.llm_manager import LLMManager
from config.config import settings

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(project_root / "tests" / "benchmark_validation.log", mode='w', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


def compare_results(generated_df: pd.DataFrame, expected_sql: str, db_adapter) -> Dict[str, Any]:
    """
    –°—Ä–∞–≤–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ SQL —Å –æ–∂–∏–¥–∞–µ–º—ã–º SQL
    
    Args:
        generated_df: DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ SQL
        expected_sql: –û–∂–∏–¥–∞–µ–º—ã–π SQL –∑–∞–ø—Ä–æ—Å
        db_adapter: Database adapter –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –æ–∂–∏–¥–∞–µ–º–æ–≥–æ SQL
        
    Returns:
        Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    """
    result = {
        'similar': False,
        'rows_match': False,
        'columns_match': False,
        'values_similar': False,
        'expected_rows': 0,
        'generated_rows': len(generated_df),
        'similarity_score': 0.0
    }
    
    try:
        # –û—á–∏—Å—Ç–∏—Ç—å –æ–∂–∏–¥–∞–µ–º—ã–π SQL (—É–±—Ä–∞—Ç—å —Ç–æ—á–∫—É —Å –∑–∞–ø—è—Ç–æ–π, –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã)
        expected_sql_clean = expected_sql.replace(';', '').strip()
        
        # –í—ã–ø–æ–ª–Ω–∏—Ç—å –æ–∂–∏–¥–∞–µ–º—ã–π SQL
        expected_df = db_adapter.execute_query(expected_sql_clean)
        result['expected_rows'] = len(expected_df)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫
        result['rows_match'] = (len(generated_df) == len(expected_df))
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–æ–Ω–æ–∫ (–∏–≥–Ω–æ—Ä–∏—Ä—É—è –ø–æ—Ä—è–¥–æ–∫)
        generated_cols = set(generated_df.columns)
        expected_cols = set(expected_df.columns)
        result['columns_match'] = (generated_cols == expected_cols)
        
        if result['rows_match'] and result['columns_match'] and len(generated_df) > 0:
            # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π (–¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫)
            numeric_cols = generated_df.select_dtypes(include=[np.number]).columns
            
            if len(numeric_cols) > 0:
                # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
                generated_sorted = generated_df.sort_values(by=list(generated_df.columns)).reset_index(drop=True)
                expected_sorted = expected_df.sort_values(by=list(expected_df.columns)).reset_index(drop=True)
                
                # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
                similarities = []
                for col in numeric_cols:
                    if col in expected_sorted.columns:
                        gen_values = generated_sorted[col].fillna(0)
                        exp_values = expected_sorted[col].fillna(0)
                        
                        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–ª–∏–∑–æ—Å—Ç—å –∑–Ω–∞—á–µ–Ω–∏–π (–≤ –ø—Ä–µ–¥–µ–ª–∞—Ö 1%)
                        if len(gen_values) > 0 and len(exp_values) > 0:
                            # –î–ª—è –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π - –ø—Ä–æ–≤–µ—Ä–∫–∞ –±–ª–∏–∑–æ—Å—Ç–∏
                            if len(gen_values) == 1 and len(exp_values) == 1:
                                diff = abs(gen_values.iloc[0] - exp_values.iloc[0])
                                max_val = max(abs(gen_values.iloc[0]), abs(exp_values.iloc[0]), 1)
                                similarity = 1 - min(diff / max_val, 1.0)
                                similarities.append(similarity)
                            else:
                                # –î–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫ - –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
                                if len(gen_values) == len(exp_values):
                                    correlation = gen_values.corr(exp_values)
                                    if pd.notna(correlation):
                                        similarities.append(max(0, correlation))
                
                if similarities:
                    result['similarity_score'] = np.mean(similarities)
                    result['values_similar'] = result['similarity_score'] > 0.8
                else:
                    # –ï—Å–ª–∏ –Ω–µ—Ç —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫, –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                    result['values_similar'] = generated_sorted.equals(expected_sorted)
                    result['similarity_score'] = 1.0 if result['values_similar'] else 0.0
            else:
                # –î–ª—è –Ω–µ—á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ - —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                generated_sorted = generated_df.sort_values(by=list(generated_df.columns)).reset_index(drop=True)
                expected_sorted = expected_df.sort_values(by=list(expected_df.columns)).reset_index(drop=True)
                result['values_similar'] = generated_sorted.equals(expected_sorted)
                result['similarity_score'] = 1.0 if result['values_similar'] else 0.0
        
        # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏
        result['similar'] = (
            result['rows_match'] and 
            result['columns_match'] and 
            (result['values_similar'] or result['similarity_score'] > 0.7)
        )
        
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}")
        result['error'] = str(e)
    
    return result


async def validate_single_example(
    orchestrator: AgentOrchestrator,
    db_adapter,
    example: Dict[str, Any],
    index: int
) -> Dict[str, Any]:
    """
    –í–∞–ª–∏–¥–∞—Ü–∏—è –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –∏–∑ –±–µ–Ω—á–º–∞—Ä–∫–∞
    
    Args:
        orchestrator: AgentOrchestrator instance
        db_adapter: Database adapter
        example: –ü—Ä–∏–º–µ—Ä –∏–∑ –±–µ–Ω—á–º–∞—Ä–∫–∞
        index: –ò–Ω–¥–µ–∫—Å –ø—Ä–∏–º–µ—Ä–∞
        
    Returns:
        Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    """
    question = example.get('question', '')
    expected_sql = example.get('sql', '')
    
    result = {
        'index': index,
        'question': question,
        'expected_sql': expected_sql,
        'worked': False,  # –°—Ä–∞–±–æ—Ç–∞–ª–æ –ª–∏ (—É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–∏–ª—Å—è –ø–∞–π–ø–ª–∞–π–Ω)
        'executed': False,  # –í—ã–ø–æ–ª–Ω–∏–ª–æ—Å—å –ª–∏ (SQL –≤—ã–ø–æ–ª–Ω–∏–ª—Å—è –±–µ–∑ –æ—à–∏–±–æ–∫)
        'similar_result': False,  # –ü–æ—Ö–æ–∂–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        'generated_sql': None,
        'error': None,
        'execution_time': 0.0,
        'rows_returned': 0,
        'comparison': {}
    }
    
    try:
        start_time = time.time()
        
        # –ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞ (–±–µ–∑ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ - –æ–Ω–∞ –±—É–¥–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º timeout –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∑–∞–≤–∏—Å–∞–Ω–∏–π
        try:
            task_result = await asyncio.wait_for(
                orchestrator.process_request(
                    user_input=question,
                    context={}
                ),
                timeout=120.0  # 2 –º–∏–Ω—É—Ç—ã –Ω–∞ –∑–∞–ø—Ä–æ—Å
            )
        except asyncio.TimeoutError:
            result['error'] = "Timeout: –∑–∞–ø—Ä–æ—Å –ø—Ä–µ–≤—ã—Å–∏–ª 2 –º–∏–Ω—É—Ç—ã"
            result['execution_time'] = time.time() - start_time
            return result
        except BrokenPipeError:
            result['error'] = "Broken pipe: –ø—Ä–æ—Ü–µ—Å—Å –±—ã–ª –ø—Ä–µ—Ä–≤–∞–Ω"
            result['execution_time'] = time.time() - start_time
            return result
        
        result['execution_time'] = time.time() - start_time
        result['worked'] = task_result.success
        
        if task_result.success and task_result.sql_result:
            sql_result = task_result.sql_result
            result['executed'] = sql_result.success
            result['generated_sql'] = sql_result.query
            result['rows_returned'] = sql_result.rows_returned if sql_result.data is not None else 0
            
            # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            if sql_result.data is not None and not sql_result.data.empty:
                comparison = compare_results(sql_result.data, expected_sql, db_adapter)
                result['comparison'] = comparison
                result['similar_result'] = comparison.get('similar', False)
            else:
                result['comparison'] = {'error': 'No data returned'}
        else:
            result['error'] = task_result.error or "Unknown error"
            
    except BrokenPipeError as e:
        logger.warning(f"Broken pipe –ø—Ä–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –ø—Ä–∏–º–µ—Ä–∞ {index}: {e}")
        result['error'] = f"Broken pipe: {str(e)}"
        result['execution_time'] = time.time() - start_time if 'start_time' in locals() else 0.0
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –ø—Ä–∏–º–µ—Ä–∞ {index}: {e}", exc_info=True)
        result['error'] = str(e)
        result['execution_time'] = time.time() - start_time if 'start_time' in locals() else 0.0
    
    return result


async def validate_benchmark(
    benchmark_path: Path,
    max_examples: Optional[int] = None,
    sample_size: Optional[int] = None
) -> Dict[str, Any]:
    """
    –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Å–µ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–∞
    
    Args:
        benchmark_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –±–µ–Ω—á–º–∞—Ä–∫–∞
        max_examples: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        sample_size: –†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è —Å–ª—É—á–∞–π–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω)
        
    Returns:
        Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    """
    logger.info(f"üìä –ó–∞–≥—Ä—É–∑–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞: {benchmark_path}")
    
    with open(benchmark_path, 'r', encoding='utf-8') as f:
        benchmark = json.load(f)
    
    total_examples = len(benchmark)
    logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {total_examples} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    # –í—ã–±–æ—Ä–∫–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    if sample_size and sample_size < total_examples:
        import random
        random.seed(42)
        examples_to_check = random.sample(benchmark, sample_size)
        logger.info(f"üé≤ –í—ã–±—Ä–∞–Ω–∞ —Å–ª—É—á–∞–π–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {sample_size} –ø—Ä–∏–º–µ—Ä–æ–≤")
    elif max_examples:
        examples_to_check = benchmark[:max_examples]
        logger.info(f"üìù –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–≤—ã—Ö {max_examples} –ø—Ä–∏–º–µ—Ä–æ–≤")
    else:
        examples_to_check = benchmark
        logger.info(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Å–µ—Ö {total_examples} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    logger.info("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤...")
    db_adapter = create_database_adapter(settings.database_url)
    llm = LLMManager(
        provider=settings.llm_provider,
        model=settings.llm_model,
        gemini_api_key=settings.gemini_api_key,
        openai_api_key=getattr(settings, 'openai_api_key', None),
        anthropic_api_key=getattr(settings, 'anthropic_api_key', None)
    )
    orchestrator = AgentOrchestrator(db_adapter, llm)
    
    logger.info("üöÄ –ù–∞—á–∞–ª–æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏...")
    
    results = []
    stats = {
        'total': len(examples_to_check),
        'worked': 0,  # –°—Ä–∞–±–æ—Ç–∞–ª–æ
        'executed': 0,  # –í—ã–ø–æ–ª–Ω–∏–ª–æ—Å—å
        'similar_result': 0,  # –ü–æ—Ö–æ–∂–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        'errors': 0
    }
    
    start_time = time.time()
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–æ–≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ (—á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å LLM)
    for i, example in enumerate(examples_to_check, 1):
        logger.info(f"\n[{i}/{len(examples_to_check)}] –í–∞–ª–∏–¥–∞—Ü–∏—è: {example.get('question', '')[:80]}...")
        
        try:
            result = await asyncio.wait_for(
                validate_single_example(orchestrator, db_adapter, example, i),
                timeout=180.0  # 3 –º–∏–Ω—É—Ç—ã –Ω–∞ –ø—Ä–∏–º–µ—Ä (–≤–∫–ª—é—á–∞—è LLM –≤—ã–∑–æ–≤—ã)
            )
            results.append(result)
        except asyncio.TimeoutError:
            logger.warning(f"‚è±Ô∏è Timeout –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞ {i}")
            results.append({
                'index': i,
                'question': example.get('question', ''),
                'error': 'Timeout: –ø—Ä–µ–≤—ã—à–µ–Ω–æ 3 –º–∏–Ω—É—Ç—ã',
                'worked': False,
                'executed': False,
                'similar_result': False
            })
            stats['errors'] += 1
        except KeyboardInterrupt:
            logger.info("\n‚ö†Ô∏è –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º (Ctrl+C)")
            break
        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–∏–º–µ—Ä–∞ {i}: {e}")
            results.append({
                'index': i,
                'question': example.get('question', ''),
                'error': f'Critical error: {str(e)}',
                'worked': False,
                'executed': False,
                'similar_result': False
            })
            stats['errors'] += 1
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        if result['worked']:
            stats['worked'] += 1
        if result['executed']:
            stats['executed'] += 1
        if result['similar_result']:
            stats['similar_result'] += 1
        if result['error']:
            stats['errors'] += 1
        
        # –ü—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 10 –ø—Ä–∏–º–µ—Ä–æ–≤
        if i % 10 == 0:
            elapsed = time.time() - start_time
            avg_time = elapsed / i
            remaining = (len(examples_to_check) - i) * avg_time
            logger.info(f"üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {i}/{len(examples_to_check)} | "
                       f"–°—Ä–∞–±–æ—Ç–∞–ª–æ: {stats['worked']} ({stats['worked']/i*100:.1f}%) | "
                       f"–í—ã–ø–æ–ª–Ω–∏–ª–æ—Å—å: {stats['executed']} ({stats['executed']/i*100:.1f}%) | "
                       f"–ü–æ—Ö–æ–∂–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {stats['similar_result']} ({stats['similar_result']/i*100:.1f}%) | "
                       f"–û—Å—Ç–∞–ª–æ—Å—å: ~{remaining/60:.1f} –º–∏–Ω")
    
    total_time = time.time() - start_time
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    final_stats = {
        **stats,
        'total_time': total_time,
        'avg_time_per_example': total_time / len(examples_to_check),
        'success_rate': {
            'worked': stats['worked'] / stats['total'] * 100,
            'executed': stats['executed'] / stats['total'] * 100,
            'similar_result': stats['similar_result'] / stats['total'] * 100
        }
    }
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    output_file = project_root / "tests" / "results" / f"benchmark_validation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    output_file.parent.mkdir(exist_ok=True)
    
    output_data = {
        'stats': final_stats,
        'results': results,
        'timestamp': datetime.now().isoformat(),
        'benchmark_file': str(benchmark_path)
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"\n{'='*80}")
    logger.info("üìä –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    logger.info(f"{'='*80}")
    logger.info(f"–í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {final_stats['total']}")
    logger.info(f"–°—Ä–∞–±–æ—Ç–∞–ª–æ (–ø–∞–π–ø–ª–∞–π–Ω —É—Å–ø–µ—à–µ–Ω): {final_stats['worked']} ({final_stats['success_rate']['worked']:.1f}%)")
    logger.info(f"–í—ã–ø–æ–ª–Ω–∏–ª–æ—Å—å (SQL –≤—ã–ø–æ–ª–Ω–µ–Ω): {final_stats['executed']} ({final_stats['success_rate']['executed']:.1f}%)")
    logger.info(f"–ü–æ—Ö–æ–∂–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {final_stats['similar_result']} ({final_stats['success_rate']['similar_result']:.1f}%)")
    logger.info(f"–û—à–∏–±–∫–∏: {final_stats['errors']}")
    logger.info(f"–û–±—â–µ–µ –≤—Ä–µ–º—è: {final_stats['total_time']/60:.1f} –º–∏–Ω")
    logger.info(f"–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ –ø—Ä–∏–º–µ—Ä: {final_stats['avg_time_per_example']:.2f} —Å–µ–∫")
    logger.info(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_file}")
    
    return output_data


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='–í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ')
    parser.add_argument('--benchmark', type=str, default='tests/benchmark_3000.json',
                       help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –±–µ–Ω—á–º–∞—Ä–∫–∞')
    parser.add_argument('--max', type=int, default=None,
                       help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏')
    parser.add_argument('--sample', type=int, default=None,
                       help='–†–∞–∑–º–µ—Ä —Å–ª—É—á–∞–π–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏')
    
    args = parser.parse_args()
    
    benchmark_path = project_root / args.benchmark
    
    if not benchmark_path.exists():
        logger.error(f"‚ùå –§–∞–π–ª –±–µ–Ω—á–º–∞—Ä–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {benchmark_path}")
        sys.exit(1)
    
    # –ó–∞–ø—É—Å–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    results = asyncio.run(validate_benchmark(
        benchmark_path=benchmark_path,
        max_examples=args.max,
        sample_size=args.sample
    ))

