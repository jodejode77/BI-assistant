"""
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ SQL –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö
"""

import json
import csv
import logging
import time
from pathlib import Path
from typing import List, Dict, Any, Tuple
import pandas as pd
from difflib import SequenceMatcher
import re

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class SQLAccuracyTester:
    """–¢–µ—Å—Ç–µ—Ä —Ç–æ—á–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ SQL"""
    
    def __init__(self, agent):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ—Å—Ç–µ—Ä–∞
        
        Args:
            agent: –≠–∫–∑–µ–º–ø–ª—è—Ä EnhancedSQLAgent
        """
        self.agent = agent
        self.results = []
    
    def normalize_sql(self, sql: str) -> str:
        """
        –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è SQL –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        
        Args:
            sql: SQL –∑–∞–ø—Ä–æ—Å
            
        Returns:
            –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π SQL
        """
        if not sql:
            return ""
        
        # –£–¥–∞–ª–∏—Ç—å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
        sql = re.sub(r'--.*', '', sql)
        sql = re.sub(r'/\*.*?\*/', '', sql, flags=re.DOTALL)
        
        # –ü—Ä–∏–≤–µ—Å—Ç–∏ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        sql = sql.lower()
        
        # –£–¥–∞–ª–∏—Ç—å –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        sql = re.sub(r'\s+', ' ', sql)
        
        # –£–¥–∞–ª–∏—Ç—å —Ç–æ—á–∫–∏ —Å –∑–∞–ø—è—Ç–æ–π
        sql = sql.replace(';', '')
        
        # –£–¥–∞–ª–∏—Ç—å –∫–∞–≤—ã—á–∫–∏ –≤–æ–∫—Ä—É–≥ –∑–Ω–∞—á–µ–Ω–∏–π
        sql = re.sub(r"'([^']*)'", r'\1', sql)
        sql = re.sub(r'"([^"]*)"', r'\1', sql)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø—Ä–æ–±–µ–ª—ã –≤–æ–∫—Ä—É–≥ –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤
        sql = re.sub(r'\s*=\s*', ' = ', sql)
        sql = re.sub(r'\s*>\s*', ' > ', sql)
        sql = re.sub(r'\s*<\s*', ' < ', sql)
        sql = re.sub(r'\s*>=\s*', ' >= ', sql)
        sql = re.sub(r'\s*<=\s*', ' <= ', sql)
        
        return sql.strip()
    
    def compare_sql(self, generated: str, expected: str) -> Dict[str, Any]:
        """
        –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–≤—É—Ö SQL –∑–∞–ø—Ä–æ—Å–æ–≤
        
        Args:
            generated: –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π SQL
            expected: –û–∂–∏–¥–∞–µ–º—ã–π SQL
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        """
        gen_norm = self.normalize_sql(generated)
        exp_norm = self.normalize_sql(expected)
        
        # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        exact_match = gen_norm == exp_norm
        
        # –°—Ö–æ–∂–µ—Å—Ç—å –ø–æ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω—É
        similarity = SequenceMatcher(None, gen_norm, exp_norm).ratio()
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        gen_tables = set(re.findall(r'from\s+(\w+)', gen_norm, re.IGNORECASE))
        exp_tables = set(re.findall(r'from\s+(\w+)', exp_norm, re.IGNORECASE))
        
        gen_columns = set(re.findall(r'select\s+(.*?)\s+from', gen_norm, re.IGNORECASE | re.DOTALL))
        exp_columns = set(re.findall(r'select\s+(.*?)\s+from', exp_norm, re.IGNORECASE | re.DOTALL))
        
        gen_where = re.search(r'where\s+(.*?)(?:group|order|limit|$)', gen_norm, re.IGNORECASE | re.DOTALL)
        exp_where = re.search(r'where\s+(.*?)(?:group|order|limit|$)', exp_norm, re.IGNORECASE | re.DOTALL)
        
        gen_where_clause = gen_where.group(1).strip() if gen_where else ""
        exp_where_clause = exp_where.group(1).strip() if exp_where else ""
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        tables_match = gen_tables == exp_tables
        where_similarity = SequenceMatcher(None, gen_where_clause, exp_where_clause).ratio() if gen_where_clause or exp_where_clause else 1.0
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–≥—Ä–µ–≥–∞—Ü–∏–π
        gen_has_agg = bool(re.search(r'\b(avg|sum|count|min|max)\b', gen_norm, re.IGNORECASE))
        exp_has_agg = bool(re.search(r'\b(avg|sum|count|min|max)\b', exp_norm, re.IGNORECASE))
        agg_match = gen_has_agg == exp_has_agg
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ GROUP BY
        gen_has_group = 'group by' in gen_norm
        exp_has_group = 'group by' in exp_norm
        group_match = gen_has_group == exp_has_group
        
        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å (–ø—Ä–æ–≤–µ—Ä–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è)
        semantic_correct = None  # –ë—É–¥–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏
        
        return {
            'exact_match': exact_match,
            'similarity': similarity,
            'tables_match': tables_match,
            'where_similarity': where_similarity,
            'agg_match': agg_match,
            'group_match': group_match,
            'generated_tables': list(gen_tables),
            'expected_tables': list(exp_tables),
            'generated_sql': generated,
            'expected_sql': expected,
            'generated_normalized': gen_norm,
            'expected_normalized': exp_norm,
            'semantic_correct': semantic_correct
        }
    
    def test_single_query(
        self,
        question: str,
        expected_sql: str,
        test_id: int = None
    ) -> Dict[str, Any]:
        """
        –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        
        Args:
            question: –í–æ–ø—Ä–æ—Å –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ
            expected_sql: –û–∂–∏–¥–∞–µ–º—ã–π SQL
            test_id: ID —Ç–µ—Å—Ç–∞
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç —Ç–µ—Å—Ç–∞
        """
        start_time = time.time()
        
        try:
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è SQL
            result = self.agent.process_question(question)
            
            generation_time = time.time() - start_time
            
            if not result.success:
                return {
                    'test_id': test_id,
                    'question': question,
                    'success': False,
                    'error': result.error,
                    'generation_time': generation_time,
                    'expected_sql': expected_sql,
                    'generated_sql': None,
                    'comparison': None
                }
            
            # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ SQL
            comparison = self.compare_sql(result.query, expected_sql)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏ (–≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞)
            try:
                # –û—á–∏—Å—Ç–∏—Ç—å SQL –æ—Ç —Ç–æ—á–∫–∏ —Å –∑–∞–ø—è—Ç–æ–π –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
                clean_gen_sql = result.query.replace(';', '').strip()
                clean_exp_sql = expected_sql.replace(';', '').strip()
                
                # –ü—ã—Ç–∞–µ–º—Å—è –≤—ã–ø–æ–ª–Ω–∏—Ç—å –æ–±–∞ –∑–∞–ø—Ä–æ—Å–∞ –∏ —Å—Ä–∞–≤–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                gen_data = self.agent.db_adapter.execute_query(clean_gen_sql)
                exp_data = self.agent.db_adapter.execute_query(clean_exp_sql)
                
                # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (—É–ø—Ä–æ—â–µ–Ω–Ω–æ–µ - –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É)
                # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –∏ –∫–æ–ª–æ–Ω–æ–∫
                rows_match = abs(len(gen_data) - len(exp_data)) <= 1  # –î–æ–ø—É—Å–∫–∞–µ–º —Ä–∞–∑–Ω–∏—Ü—É –≤ 1 —Å—Ç—Ä–æ–∫—É
                cols_match = gen_data.columns.tolist() == exp_data.columns.tolist()
                
                # –ï—Å–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–æ–≤–ø–∞–¥–∞–µ—Ç, –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –ø–æ—Ö–æ–∂–∏
                if rows_match and cols_match and len(gen_data) > 0:
                    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫ (–µ—Å–ª–∏ –µ—Å—Ç—å)
                    try:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–∞—é—Ç
                        gen_dtypes = gen_data.dtypes.to_dict()
                        exp_dtypes = exp_data.dtypes.to_dict()
                        dtypes_match = gen_dtypes == exp_dtypes
                        
                        semantic_correct = rows_match and cols_match and dtypes_match
                    except:
                        semantic_correct = rows_match and cols_match
                else:
                    semantic_correct = False
                
                comparison['semantic_correct'] = semantic_correct
                comparison['generated_rows'] = len(gen_data)
                comparison['expected_rows'] = len(exp_data)
                
            except Exception as e:
                logger.debug(f"Semantic check failed: {e}")
                comparison['semantic_correct'] = False
                comparison['semantic_error'] = str(e)
            
            return {
                'test_id': test_id,
                'question': question,
                'success': True,
                'generation_time': generation_time,
                'expected_sql': expected_sql,
                'generated_sql': result.query,
                'comparison': comparison,
                'retry_count': result.retry_count,
                'confidence': result.generation_metadata.confidence if result.generation_metadata else None
            }
            
        except Exception as e:
            logger.error(f"Error testing query {test_id}: {e}", exc_info=True)
            return {
                'test_id': test_id,
                'question': question,
                'success': False,
                'error': str(e),
                'generation_time': time.time() - start_time,
                'expected_sql': expected_sql,
                'generated_sql': None,
                'comparison': None
            }
    
    def test_from_csv(self, csv_path: str, max_tests: int = None) -> Dict[str, Any]:
        """
        –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ CSV –¥–∞—Ç–∞—Å–µ—Ç–µ
        
        Args:
            csv_path: –ü—É—Ç—å –∫ CSV —Ñ–∞–π–ª—É
            max_tests: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ—Å—Ç–æ–≤ (None = –≤—Å–µ)
            
        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        """
        logger.info(f"Loading CSV dataset from {csv_path}")
        
        test_cases = []
        with open(csv_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for i, row in enumerate(reader):
                if max_tests and i >= max_tests:
                    break
                test_cases.append({
                    'id': i + 1,
                    'question': row['Question'],
                    'expected_sql': row['SQL Query'],
                    'viz_type': row.get('Visualization Type', '')
                })
        
        logger.info(f"Loaded {len(test_cases)} test cases")
        return self._run_tests(test_cases, "CSV")
    
    def test_from_json(self, json_path: str, max_tests: int = None) -> Dict[str, Any]:
        """
        –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ JSON –¥–∞—Ç–∞—Å–µ—Ç–µ
        
        Args:
            json_path: –ü—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É
            max_tests: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ—Å—Ç–æ–≤ (None = –≤—Å–µ)
            
        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        """
        logger.info(f"Loading JSON dataset from {json_path}")
        
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        test_cases = []
        for i, item in enumerate(data):
            if max_tests and i >= max_tests:
                break
            test_cases.append({
                'id': i + 1,
                'question': item['natural_language_query'],
                'expected_sql': item['sql_query'],
                'viz_type': item.get('visualization_type', '')
            })
        
        logger.info(f"Loaded {len(test_cases)} test cases")
        return self._run_tests(test_cases, "JSON")
    
    def _run_tests(self, test_cases: List[Dict], dataset_type: str) -> Dict[str, Any]:
        """
        –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤
        
        Args:
            test_cases: –°–ø–∏—Å–æ–∫ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å–ª—É—á–∞–µ–≤
            dataset_type: –¢–∏–ø –¥–∞—Ç–∞—Å–µ—Ç–∞
            
        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        """
        logger.info(f"Running {len(test_cases)} tests from {dataset_type} dataset...")
        
        results = []
        start_time = time.time()
        
        for i, test_case in enumerate(test_cases):
            if (i + 1) % 10 == 0:
                logger.info(f"Progress: {i + 1}/{len(test_cases)} tests completed")
            
            result = self.test_single_query(
                question=test_case['question'],
                expected_sql=test_case['expected_sql'],
                test_id=test_case['id']
            )
            results.append(result)
        
        total_time = time.time() - start_time
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        stats = self._calculate_statistics(results, dataset_type, total_time)
        
        return stats
    
    def _calculate_statistics(
        self,
        results: List[Dict],
        dataset_type: str,
        total_time: float
    ) -> Dict[str, Any]:
        """
        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        
        Args:
            results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–æ–≤
            dataset_type: –¢–∏–ø –¥–∞—Ç–∞—Å–µ—Ç–∞
            total_time: –û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
            
        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        """
        total = len(results)
        successful = sum(1 for r in results if r.get('success', False))
        failed = total - successful
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —É—Å–ø–µ—à–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤
        successful_results = [r for r in results if r.get('success', False)]
        
        if successful_results:
            exact_matches = sum(
                1 for r in successful_results
                if r.get('comparison', {}).get('exact_match', False)
            )
            
            similarities = [
                r.get('comparison', {}).get('similarity', 0)
                for r in successful_results
                if r.get('comparison')
            ]
            
            tables_matches = sum(
                1 for r in successful_results
                if r.get('comparison', {}).get('tables_match', False)
            )
            
            semantic_correct = sum(
                1 for r in successful_results
                if r.get('comparison', {}).get('semantic_correct', False)
            )
            
            avg_similarity = sum(similarities) / len(similarities) if similarities else 0
            avg_time = sum(r.get('generation_time', 0) for r in successful_results) / len(successful_results)
            avg_retries = sum(r.get('retry_count', 0) for r in successful_results) / len(successful_results)
            
            confidences = [
                r.get('confidence', 0) for r in successful_results
                if r.get('confidence') is not None
            ]
            avg_confidence = sum(confidences) / len(confidences) if confidences else 0
        else:
            exact_matches = 0
            avg_similarity = 0
            tables_matches = 0
            semantic_correct = 0
            avg_time = 0
            avg_retries = 0
            avg_confidence = 0
        
        stats = {
            'dataset_type': dataset_type,
            'total_tests': total,
            'successful': successful,
            'failed': failed,
            'success_rate': successful / total if total > 0 else 0,
            'exact_matches': exact_matches,
            'exact_match_rate': exact_matches / successful if successful > 0 else 0,
            'avg_similarity': avg_similarity,
            'tables_match_rate': tables_matches / successful if successful > 0 else 0,
            'semantic_correct_rate': semantic_correct / successful if successful > 0 else 0,
            'avg_generation_time': avg_time,
            'avg_retries': avg_retries,
            'avg_confidence': avg_confidence,
            'total_time': total_time,
            'throughput': total / total_time if total_time > 0 else 0,
            'results': results
        }
        
        return stats
    
    def print_statistics(self, stats: Dict[str, Any]):
        """
        –í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        
        Args:
            stats: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        """
        print("\n" + "="*80)
        print(f"üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø - {stats['dataset_type']} Dataset")
        print("="*80)
        print(f"\nüìà –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   –í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤: {stats['total_tests']}")
        print(f"   –£—Å–ø–µ—à–Ω—ã—Ö: {stats['successful']} ({stats['success_rate']*100:.1f}%)")
        print(f"   –ù–µ—É–¥–∞—á–Ω—ã—Ö: {stats['failed']} ({(1-stats['success_rate'])*100:.1f}%)")
        
        print(f"\nüéØ –¢–æ—á–Ω–æ—Å—Ç—å SQL:")
        print(f"   –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ: {stats['exact_matches']} ({stats['exact_match_rate']*100:.1f}%)")
        print(f"   –°—Ä–µ–¥–Ω—è—è —Å—Ö–æ–∂–µ—Å—Ç—å: {stats['avg_similarity']*100:.1f}%")
        print(f"   –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü: {stats['tables_match_rate']*100:.1f}%")
        print(f"   –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å: {stats['semantic_correct_rate']*100:.1f}%")
        
        print(f"\n‚ö° –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:")
        print(f"   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {stats['avg_generation_time']:.2f}s")
        print(f"   –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ retry: {stats['avg_retries']:.2f}")
        print(f"   –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {stats['avg_confidence']*100:.1f}%")
        print(f"   –û–±—â–µ–µ –≤—Ä–µ–º—è: {stats['total_time']:.1f}s")
        print(f"   –ü—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å: {stats['throughput']:.2f} –∑–∞–ø—Ä–æ—Å–æ–≤/—Å–µ–∫")
        
        print("\n" + "="*80)
    
    def save_results(self, stats: Dict[str, Any], output_path: str):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
        Args:
            stats: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        """
        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–æ–ª–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"Results saved to {output_file}")
        
        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫—Ä–∞—Ç–∫—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        summary_file = output_file.with_suffix('.summary.json')
        summary = {
            'dataset_type': stats['dataset_type'],
            'total_tests': stats['total_tests'],
            'success_rate': stats['success_rate'],
            'exact_match_rate': stats['exact_match_rate'],
            'avg_similarity': stats['avg_similarity'],
            'tables_match_rate': stats['tables_match_rate'],
            'semantic_correct_rate': stats['semantic_correct_rate'],
            'avg_generation_time': stats['avg_generation_time'],
            'avg_retries': stats['avg_retries'],
            'avg_confidence': stats['avg_confidence']
        }
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Summary saved to {summary_file}")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Ç–µ—Å—Ç–æ–≤"""
    import sys
    from pathlib import Path
    
    # –î–æ–±–∞–≤–∏—Ç—å —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
    project_root = Path(__file__).parent.parent
    sys.path.insert(0, str(project_root))
    
    from agents.enhanced_sql_agent import create_universal_sql_agent
    from core.llm_manager import LLMManager
    from config.config import settings
    
    print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã...")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ LLM –º–µ–Ω–µ–¥–∂–µ—Ä–∞
    provider_key_map = {
        "gemini": "gemini_api_key",
        "openai": "openai_api_key",
        "anthropic": "anthropic_api_key"
    }
    api_key_name = provider_key_map.get(settings.llm_provider, "gemini_api_key")
    api_key = getattr(settings, api_key_name, None)
    
    llm = LLMManager(
        provider=settings.llm_provider,
        model=settings.llm_model,
        gemini_api_key=api_key if settings.llm_provider == "gemini" else None,
        openai_api_key=api_key if settings.llm_provider == "openai" else None,
        anthropic_api_key=api_key if settings.llm_provider == "anthropic" else None
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞
    print("üì° –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö...")
    agent = create_universal_sql_agent(
        connection_url=settings.database_url,
        llm_manager=llm,
        enable_analysis=True,
        max_retries=3
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–µ—Ä–∞
    tester = SQLAccuracyTester(agent)
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ CSV –¥–∞—Ç–∞—Å–µ—Ç–µ
    csv_path = project_root / "home_credit_qa_11000_with_hard_joins.csv"
    if csv_path.exists():
        print(f"\nüìã –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ CSV –¥–∞—Ç–∞—Å–µ—Ç–µ (–ø–µ—Ä–≤—ã–µ 50 —Ç–µ—Å—Ç–æ–≤)...")
        csv_stats = tester.test_from_csv(str(csv_path), max_tests=50)
        tester.print_statistics(csv_stats)
        tester.save_results(csv_stats, "tests/results/csv_test_results.json")
    else:
        print(f"‚ö†Ô∏è  CSV —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {csv_path}")
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ JSON –¥–∞—Ç–∞—Å–µ—Ç–µ
    json_path = project_root / "result10000.json"
    if json_path.exists():
        print(f"\nüìã –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ JSON –¥–∞—Ç–∞—Å–µ—Ç–µ (–ø–µ—Ä–≤—ã–µ 50 —Ç–µ—Å—Ç–æ–≤)...")
        json_stats = tester.test_from_json(str(json_path), max_tests=50)
        tester.print_statistics(json_stats)
        tester.save_results(json_stats, "tests/results/json_test_results.json")
    else:
        print(f"‚ö†Ô∏è  JSON —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {json_path}")
    
    print("\n‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")


if __name__ == "__main__":
    main()

