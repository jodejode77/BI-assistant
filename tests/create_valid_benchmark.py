"""
–°–æ–∑–¥–∞–Ω–∏–µ –±–µ–Ω—á–º–∞—Ä–∫–∞ –∏–∑ –≤–∞–ª–∏–¥–Ω—ã—Ö SQL –∑–∞–ø—Ä–æ—Å–æ–≤
–° —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω—ã–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –ø–æ —Ç–∏–ø–∞–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ 4500 –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ —Å–æ–∑–¥–∞–µ—Ç –±–µ–Ω—á–º–∞—Ä–∫ –∏–∑ 3000 –≤–∞–ª–∏–¥–Ω—ã—Ö
"""

import json
import csv
import logging
import time
from pathlib import Path
from typing import Dict, Any, List
import sys
import random
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError as FutureTimeoutError
from threading import Lock

# –î–æ–±–∞–≤–∏—Ç—å —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from core.database_adapter import create_database_adapter
from config.config import settings

# –û—Ç–∫–ª—é—á–∏—Ç—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–æ–∫ –æ—Ç database_adapter
logging.getLogger('core.database_adapter').setLevel(logging.CRITICAL)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ —Ñ–∞–π–ª
log_file = project_root / "tests" / "benchmark_creation.log"
file_handler = logging.FileHandler(log_file, mode='w', encoding='utf-8')
file_handler.setLevel(logging.INFO)
file_handler.setFormatter(logging.Formatter('%(asctime)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S'))

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –∫–æ–Ω—Å–æ–ª—å
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_handler.setFormatter(logging.Formatter('%(message)s'))

# –°–æ–∑–¥–∞—Ç—å logger –∏ –¥–æ–±–∞–≤–∏—Ç—å –æ–±–∞ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
logger.addHandler(file_handler)
logger.addHandler(console_handler)
logger.propagate = False  # –ù–µ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å –≤ –∫–æ—Ä–Ω–µ–≤–æ–π logger

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —Å—á–µ—Ç—á–∏–∫ –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
progress_lock = Lock()
progress_counter = {'processed': 0, 'valid': 0}


def validate_sql_query(db_adapter, sql: str) -> Dict[str, Any]:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è SQL –∑–∞–ø—Ä–æ—Å–∞"""
    sql = sql.replace(';', '').strip()
    
    result = {
        'valid': False,
        'executable': False,
        'error': None
    }
    
    # –°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
    try:
        is_valid, error = db_adapter.validate_query(sql)
        result['valid'] = is_valid
        if not is_valid:
            result['error'] = error
            return result
    except Exception as e:
        result['error'] = f"Validation error: {str(e)}"
        return result
    
    # –ü–æ–ø—ã—Ç–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    try:
        data = db_adapter.execute_query(sql)
        result['executable'] = True
    except Exception as e:
        result['error'] = f"Execution error: {str(e)}"
        result['executable'] = False
    
    return result


def load_all_datasets(max_queries: int = 4500) -> List[Dict[str, Any]]:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å–µ –¥–∞—Ç–∞—Å–µ—Ç—ã (CSV –∏ JSON)"""
    datasets = []
    
    # CSV –¥–∞—Ç–∞—Å–µ—Ç
    csv_path = project_root / "home_credit_qa_11000_with_hard_joins.csv"
    if csv_path.exists():
        print(f"üìã –ó–∞–≥—Ä—É–∑–∫–∞ CSV –¥–∞—Ç–∞—Å–µ—Ç–∞: {csv_path}...")
        with open(csv_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                datasets.append({
                    'question': row['Question'],
                    'sql': row['SQL Query'],
                    'visualization_type': row.get('Visualization Type', 'table').lower().strip(),
                    'source': 'csv'
                })
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(datasets)} –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ CSV")
    else:
        print(f"‚ö†Ô∏è  CSV —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {csv_path}")
    
    # JSON –¥–∞—Ç–∞—Å–µ—Ç
    json_path = project_root / "result10000.json"
    if json_path.exists():
        print(f"üìã –ó–∞–≥—Ä—É–∑–∫–∞ JSON –¥–∞—Ç–∞—Å–µ—Ç–∞: {json_path}...")
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            initial_count = len(datasets)
            for item in data:
                datasets.append({
                    'question': item.get('natural_language_query', ''),
                    'sql': item.get('sql_query', ''),
                    'visualization_type': item.get('visualization_type', 'table').lower().strip(),
                    'source': 'json'
                })
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(datasets) - initial_count} –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ JSON")
    else:
        print(f"‚ö†Ô∏è  JSON —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {json_path}")
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–∏–ø–æ–≤ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    type_mapping = {
        'pie': 'pie',
        'bar': 'bar',
        'line': 'line',
        'table': 'table',
        'scatter': 'table',
        'histogram': 'bar',
        'area': 'line'
    }
    
    for item in datasets:
        viz_type = item['visualization_type']
        item['visualization_type'] = type_mapping.get(viz_type, 'table')
    
    # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞
    if len(datasets) > max_queries:
        print(f"‚ö†Ô∏è  –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–æ {max_queries} –∑–∞–ø—Ä–æ—Å–æ–≤ (–±—ã–ª–æ {len(datasets)})")
        random.seed(42)
        datasets = random.sample(datasets, max_queries)
    
    print(f"\n‚úÖ –í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {len(datasets)} –ø—Ä–∏–º–µ—Ä–æ–≤\n")
    return datasets


def validate_single_query(args):
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –æ–¥–Ω–æ–≥–æ SQL –∑–∞–ø—Ä–æ—Å–∞ (–¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏)"""
    test_case, db_adapter, total_count = args
    sql = test_case.get('sql', '').strip()
    
    if not sql:
        return None, False
    
    # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–±—â–∏–π –∞–¥–∞–ø—Ç–µ—Ä (SQLAlchemy engine thread-safe)
    validation_result = validate_sql_query(db_adapter, sql)
    
    # –û–±–Ω–æ–≤–∏—Ç—å —Å—á–µ—Ç—á–∏–∫ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
    global progress_counter, progress_lock
    with progress_lock:
        progress_counter['processed'] += 1
        processed = progress_counter['processed']
        if validation_result['valid'] and validation_result['executable']:
            progress_counter['valid'] += 1
        valid_count = progress_counter['valid']
    
    # –í—ã–≤–æ–¥–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 100 –∑–∞–ø—Ä–æ—Å–æ–≤
    if processed % 100 == 0:
        msg = f"üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {processed}/{total_count} –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ | –í–∞–ª–∏–¥–Ω—ã—Ö: {valid_count} ({valid_count/processed*100:.1f}%)"
        print(msg, flush=True)
        logger.info(msg)
    
    is_valid = validation_result['valid'] and validation_result['executable']
    if is_valid:
        test_case['validated'] = True
        test_case['validation_error'] = None
        return test_case, True
    else:
        test_case['validated'] = False
        test_case['validation_error'] = validation_result.get('error')
        return None, False


def validate_all_queries(queries: List[Dict[str, Any]], max_workers: int = 8, max_queries: int = None) -> List[Dict[str, Any]]:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Å–µ—Ö SQL –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
    if max_queries and max_queries < len(queries):
        queries = queries[:max_queries]
        print(f"üìã –ù–∞—á–∞–ª–æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ {len(queries)} –∑–∞–ø—Ä–æ—Å–æ–≤ (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–æ –¥–æ {max_queries})...")
    else:
        print(f"üìã –ù–∞—á–∞–ª–æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ {len(queries)} –∑–∞–ø—Ä–æ—Å–æ–≤...")
    print(f"‚ö° –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {max_workers} –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è\n")
    
    # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    print("üì° –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö...")
    test_adapter = create_database_adapter(settings.database_url)
    print(f"‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ {test_adapter.dialect.value}\n")
    
    print("üîç –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Å–µ—Ö SQL –∑–∞–ø—Ä–æ—Å–æ–≤ (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ)...\n")
    
    valid_queries = []
    start_time = time.time()
    
    # –°–±—Ä–æ—Å–∏—Ç—å —Å—á–µ—Ç—á–∏–∫ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
    global progress_counter
    progress_counter = {'processed': 0, 'valid': 0}
    
    # –°–æ–∑–¥–∞—Ç—å –æ–¥–∏–Ω –æ–±—â–∏–π –∞–¥–∞–ø—Ç–µ—Ä –¥–ª—è –≤—Å–µ—Ö –ø–æ—Ç–æ–∫–æ–≤ (SQLAlchemy engine thread-safe)
    shared_adapter = create_database_adapter(settings.database_url)
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
    args_list = [(test_case, shared_adapter, len(queries)) for test_case in queries]
    
    # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è —Å —Ä–∞–Ω–Ω–∏–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ–º
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(validate_single_query, args): args[0] for args in args_list}
        
        completed = 0
        last_progress_time = time.time()
        stuck_threshold = 300  # 5 –º–∏–Ω—É—Ç –±–µ–∑ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ = –∑–∞–≤–∏—Å–∞–Ω–∏–µ
        
        for future in as_completed(futures, timeout=600):  # –û–±—â–∏–π —Ç–∞–π–º–∞—É—Ç 10 –º–∏–Ω—É—Ç
            try:
                result, is_valid = future.result(timeout=20)  # –£–º–µ–Ω—å—à–µ–Ω —Ç–∞–π–º–∞—É—Ç –¥–æ 20 —Å–µ–∫
                if is_valid and result:
                    valid_queries.append(result)
                completed += 1
                last_progress_time = time.time()
                
                # –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ 500 –∑–∞–ø—Ä–æ—Å–æ–≤
                if completed % 100 == 0 and valid_queries:
                    logger.info(f"–ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: {len(valid_queries)} –≤–∞–ª–∏–¥–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ —Å–æ–±—Ä–∞–Ω–æ")
                
                # –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 100 –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 200
                if completed >= len(queries) - 200 and completed % 100 == 0:
                    logger.info(f"–ü—Ä–æ–≥—Ä–µ—Å—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {completed}/{len(queries)}, –≤–∞–ª–∏–¥–Ω—ã—Ö: {len(valid_queries)}")
                
                # –†–∞–Ω–Ω–µ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –µ—Å–ª–∏ –æ—Å—Ç–∞–ª–æ—Å—å < 100 –∏ —É–∂–µ –µ—Å—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤–∞–ª–∏–¥–Ω—ã—Ö
                remaining = len(queries) - completed
                if remaining < 100 and len(valid_queries) >= 2500:
                    logger.info(f"–†–∞–Ω–Ω–µ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ: –æ—Å—Ç–∞–ª–æ—Å—å {remaining} –∑–∞–ø—Ä–æ—Å–æ–≤, –Ω–æ —É–∂–µ —Å–æ–±—Ä–∞–Ω–æ {len(valid_queries)} –≤–∞–ª–∏–¥–Ω—ã—Ö (–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ)")
                    # –û—Ç–º–µ–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –∑–∞–¥–∞—á–∏
                    for f in futures:
                        if not f.done():
                            f.cancel()
                    break
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∑–∞–≤–∏—Å–∞–Ω–∏–µ
                if time.time() - last_progress_time > stuck_threshold:
                    logger.warning(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –∑–∞–≤–∏—Å–∞–Ω–∏–µ: –Ω–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ {stuck_threshold} —Å–µ–∫—É–Ω–¥")
                    logger.info(f"–ó–∞–≤–µ—Ä—à–∞–µ–º —Å {completed}/{len(queries)} –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ, {len(valid_queries)} –≤–∞–ª–∏–¥–Ω—ã—Ö")
                    # –û—Ç–º–µ–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –∑–∞–¥–∞—á–∏
                    for f in futures:
                        if not f.done():
                            f.cancel()
                    break
                    
            except FutureTimeoutError:
                completed += 1
                logger.warning(f"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–∞ (–æ–±—â–∏–π —Ç–∞–π–º–∞—É—Ç)")
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∑–∞–≤–∏—Å–ª–∏ –ª–∏ –º—ã
                if time.time() - last_progress_time > stuck_threshold:
                    logger.warning(f"–ó–∞–≤–∏—Å–∞–Ω–∏–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ, –∑–∞–≤–µ—Ä—à–∞–µ–º")
                    break
            except Exception as e:
                completed += 1
                if "timeout" in str(e).lower() or "Timeout" in str(type(e).__name__):
                    logger.warning(f"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}")
                else:
                    logger.debug(f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
                pass
    
    total_time = time.time() - start_time
    
    print(f"\n‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {total_time:.1f}s")
    print(f"üìä –ù–∞–π–¥–µ–Ω–æ –≤–∞–ª–∏–¥–Ω—ã—Ö SQL: {len(valid_queries)} –∏–∑ {len(queries)} ({len(valid_queries)/len(queries)*100:.1f}%)")
    print(f"‚ö° –°–∫–æ—Ä–æ—Å—Ç—å: {len(queries)/total_time:.1f} –∑–∞–ø—Ä–æ—Å–æ–≤/—Å–µ–∫\n")
    
    return valid_queries


def create_balanced_benchmark(
    valid_queries: List[Dict[str, Any]],
    target_size: int = 500,
    seed: int = 42
) -> List[Dict[str, Any]]:
    """
    –°–æ–∑–¥–∞—Ç—å —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ —Å —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω—ã–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –ø–æ —Ç–∏–ø–∞–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    
    Args:
        valid_queries: –°–ø–∏—Å–æ–∫ –≤–∞–ª–∏–¥–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        target_size: –¶–µ–ª–µ–≤–æ–π —Ä–∞–∑–º–µ—Ä –±–µ–Ω—á–º–∞—Ä–∫–∞
        seed: Seed –¥–ª—è —Ä–∞–Ω–¥–æ–º–∏–∑–∞—Ü–∏–∏
    """
    print(f"üìä –°–æ–∑–¥–∞–Ω–∏–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–∞ –∏–∑ {target_size} –ø—Ä–∏–º–µ—Ä–æ–≤...\n")
    
    random.seed(seed)
    
    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Ç–∏–ø–∞–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    by_viz_type = {}
    for query in valid_queries:
        viz_type = query.get('visualization_type', 'table')
        if viz_type not in by_viz_type:
            by_viz_type[viz_type] = []
        by_viz_type[viz_type].append(query)
    
    print("üìà –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ç–∏–ø—ã –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏:")
    for viz_type, queries in sorted(by_viz_type.items()):
        print(f"   {viz_type}: {len(queries)} –≤–∞–ª–∏–¥–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")
    
    # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –Ω–∞ —Ç–∏–ø
    num_types = len(by_viz_type)
    if num_types == 0:
        print("‚ùå –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤!")
        return []
    
    examples_per_type = target_size // num_types
    remainder = target_size % num_types
    
    print(f"\nüéØ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:")
    print(f"   –ü—Ä–∏–º–µ—Ä–æ–≤ –Ω–∞ —Ç–∏–ø: {examples_per_type}")
    if remainder > 0:
        print(f"   –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö: {remainder}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–∞
    benchmark = []
    
    for i, (viz_type, queries) in enumerate(sorted(by_viz_type.items())):
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–ª—è —ç—Ç–æ–≥–æ —Ç–∏–ø–∞
        count = examples_per_type
        if i < remainder:  # –†–∞—Å–ø—Ä–µ–¥–µ–ª–∏—Ç—å –æ—Å—Ç–∞—Ç–æ–∫
            count += 1
        
        # –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º
        count = min(count, len(queries))
        
        # –°–ª—É—á–∞–π–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞
        selected = random.sample(queries, count)
        benchmark.extend(selected)
        
        print(f"   {viz_type}: {count} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏–µ
    random.shuffle(benchmark)
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"\n‚úÖ –°–æ–∑–¥–∞–Ω –±–µ–Ω—á–º–∞—Ä–∫: {len(benchmark)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    final_distribution = {}
    for query in benchmark:
        viz_type = query.get('visualization_type', 'table')
        final_distribution[viz_type] = final_distribution.get(viz_type, 0) + 1
    
    print(f"\nüìä –§–∏–Ω–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:")
    for viz_type, count in sorted(final_distribution.items()):
        print(f"   {viz_type}: {count} ({count/len(benchmark)*100:.1f}%)")
    
    return benchmark


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    output_path = project_root / "tests" / "benchmark_3000_valid.json"
    log_path = project_root / "tests" / "benchmark_creation.log"
    
    msg = "üöÄ –°–æ–∑–¥–∞–Ω–∏–µ –±–µ–Ω—á–º–∞—Ä–∫–∞ –∏–∑ –≤–∞–ª–∏–¥–Ω—ã—Ö SQL –∑–∞–ø—Ä–æ—Å–æ–≤"
    print(msg)
    logger.info(msg)
    
    msg = "   –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ 4500 –∑–∞–ø—Ä–æ—Å–æ–≤, —Å–æ–∑–¥–∞–Ω–∏–µ –±–µ–Ω—á–º–∞—Ä–∫–∞ –∏–∑ 3000 –≤–∞–ª–∏–¥–Ω—ã—Ö"
    print(msg)
    logger.info(msg)
    
    msg = f"   üìù –õ–æ–≥–∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤: {log_path}"
    print(msg)
    logger.info(msg)
    
    msg = "="*80
    print(f"\n{msg}\n")
    logger.info(msg)
    
    # –®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    all_queries = load_all_datasets(max_queries=3800)
    
    # –®–∞–≥ 2: –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–æ –¥–æ 3800, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –∑–∞–≤–∏—Å–∞–Ω–∏—è –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 100)
    # –ò–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∑–∞–ø—É—Å–∫–∞: 2555 –≤–∞–ª–∏–¥–Ω—ã—Ö –∏–∑ 3800 = ~67% –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏
    # –≠—Ç–æ–≥–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–µ–Ω—á–º–∞—Ä–∫–∞
    valid_queries = validate_all_queries(all_queries, max_workers=8, max_queries=3800)
    
    if len(valid_queries) < 3000:
        print(f"‚ö†Ô∏è  –í–∞–ª–∏–¥–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –º–µ–Ω—å—à–µ 3000 ({len(valid_queries)})")
        print("–ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –≤–∞–ª–∏–¥–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã")
        target_size = len(valid_queries)
    else:
        target_size = 3000
    
    # –®–∞–≥ 3: –°–æ–∑–¥–∞–Ω–∏–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–∞
    balanced_benchmark = create_balanced_benchmark(
        valid_queries,
        target_size=target_size,
        seed=42
    )
    
    # –®–∞–≥ 4: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –±–µ–Ω—á–º–∞—Ä–∫–∞ –≤ {output_path}...")
    output_file = Path(output_path)
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(balanced_benchmark, f, indent=2, ensure_ascii=False)
    
    print(f"‚úÖ –ë–µ–Ω—á–º–∞—Ä–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {len(balanced_benchmark)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    stats_file = output_file.with_suffix('.stats.json')
    stats = {
        'total_examples': len(balanced_benchmark),
        'target_size': target_size,
        'all_valid': True,
        'visualization_distribution': {
            viz_type: sum(1 for q in balanced_benchmark if q.get('visualization_type') == viz_type)
            for viz_type in set(q.get('visualization_type', 'table') for q in balanced_benchmark)
        },
        'sources': {
            source: sum(1 for q in balanced_benchmark if q.get('source') == source)
            for source in set(q.get('source', 'unknown') for q in balanced_benchmark)
        }
    }
    
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    
    print(f"‚úÖ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {stats_file}")
    
    print("\n" + "="*80)
    print("‚úÖ –ì–æ—Ç–æ–≤–æ! –ë–µ–Ω—á–º–∞—Ä–∫ –∏–∑ –≤–∞–ª–∏–¥–Ω—ã—Ö SQL —Å–æ–∑–¥–∞–Ω")
    print("="*80 + "\n")


if __name__ == "__main__":
    main()

